---
title: "Comparación de Modelos"
author: 
- Juan Sosa PhD
- Webpage https://sites.google.com/view/juansosa/ 
- YouTube https://www.youtube.com/c/JuanSosa1702 
- GitHub  https://github.com/jstats1702 
- Rpubs   https://rpubs.com/jstats1702
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Criterios de información

Las medidas de **precisión predictiva** (interna) se denominan **criterios de información** y normalmente se definen en función de la **devianza** (*deviance*):
$$-2\,\text{log}\,p(\boldsymbol{y}\mid\hat{\boldsymbol{\theta}})\,.$$

El factor $-2$ se utiliza para emular el estadístico asociado con la **prueba de razón de verosimilitud** (*Likelihood-ratio test*):
$$
\lambda = -2\,\frac{\text{sup}_{\theta\in\Theta_0} L(\theta)}{\text{sup}_{\theta\in\Theta} L(\theta)}
$$
donde $L(\cdot)$ es la función de verosimilitud.

No se debe elegir necesariamente elegir el modelo con la devianza más baja. Es necesario **hacer una corrección** de cuánto aumentará la precisión predictiva (ajuste del modelo) teniendo en cuenta el **número de parámetros**.

Es de interés la **precisión de la predicción** por dos razones:

- Cuantificar el rendimiento del modelo.
- Comparar modelos. 

Hay varios **métodos disponibles para estimar la precisión predictiva** esperada sin utilizar datos fuera de la muestra (validación cruzada).

***Devieance Information Criterion*** (DIC) es una versión Bayesiana del ***Akaike Infomation Criterion*** (AIC, $\text{AIC}=-2\,\text{log}\,p(\boldsymbol{y}\mid\hat{\boldsymbol{\theta}}_{\text{MLE}})+2k$, con $k$ el número de parámetros):
$$
\text{DIC} = -2\,\text{log}\,p(\boldsymbol{y}\mid\hat{\boldsymbol{\theta}}_{\text{Bayes}}) + 2p_{\text{DIC}}
$$
donde 
$$
\hat{\boldsymbol{\theta}}_{\text{Bayes}} = \textsf{E}(\boldsymbol{\theta}\mid\boldsymbol{y})\approx\frac{1}{B}\sum_{b=1}^B \boldsymbol{\theta}^{(b)}
$$
es la media posterior de $\boldsymbol{\theta}$ y 
$$
p_{\text{DIC}} = 2\left( \text{log}\,p(\boldsymbol{y}\mid\hat{\boldsymbol{\theta}}_{\text{Bayes}}) - \textsf{E}( \text{log}\,p(\boldsymbol{y}\mid\boldsymbol{\theta})  \mid \boldsymbol{y} ) \right) \approx 2\left( \text{log}\,p(\boldsymbol{y}\mid\hat{\boldsymbol{\theta}}_{\text{Bayes}}) - \frac{1}{B}\sum_{b=1}^B \text{log}\,p\left(\boldsymbol{y}\mid \boldsymbol{\theta}^{(b)}\right)  \right)
$$
es el **número efectivo de parámetros**.

- ***Spiegelhalter, D. J., Best, N. G., Carlin, B. P., & Van Der Linde, A. (2002). Bayesian measures of model complexity and fit. Journal of the royal statistical society: Series b (statistical methodology), 64(4), 583-639.***
- ***Spiegelhalter, D. J., Best, N. G., Carlin, B. P., & Van der Linde, A. (2014). The deviance information criterion: 12 years on. Journal of the Royal Statistical Society: Series B: Statistical Methodology, 485-493.***

Similarmente, el ***Watanabe-Akaike or widely available information criterion*** (WAIC) se define como:
$$
\text{WAIC} = -2\text{lppd} + 2p_{\text{WAIC}}
$$
donde
$$
\text{lppd} = \text{log}\prod_{i=1}^n p(y_i\mid\boldsymbol{y}) = \sum_{i=1}^n\text{log}\int_\Theta p (y_i\mid\boldsymbol{\theta})p(\boldsymbol{\theta}\mid\boldsymbol{y}) \approx \sum_{i=1}^n\text{log}\left(\frac{1}{B}\sum_{b=1}^B p\left(y_i\mid\boldsymbol{\theta^{(b)}}\right)\right)
$$
es la log densidad predictiva puntual (que como la devianza resume la precisión predictiva del modelo ajustado a los datos), y
$$
p_{\text{WAIC}} = 2\sum_{i=1}^n\left( \text{log}\,\textsf{E}(p(y_i\mid\boldsymbol{\theta})\mid\boldsymbol{y}) - \textsf{E}(\text{log}\,p(y_i\mid\boldsymbol{\theta})\mid\boldsymbol{y}) \right) \approx 
2\sum_{i=1}^n\left(\text{log}\left( \frac{1}{B}\sum_{b=1}^B p\left(y_i\mid\boldsymbol{\theta}^{(b)}\right) \right) - \frac{1}{B}\sum_{b=1}^B \text{log}\,p\left(y_i\mid \boldsymbol{\theta}^{(b)} \right)\right) 
$$
es el **número efectivo de parámetros**.

- ***Watanabe, S., & Opper, M. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of machine learning research, 11(12).***
- ***Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and computing, 24(6), 997-1016.***

El WAIC es un enfoque más Bayesiano para estimar la precisión predictiva fuera de la muestra, comenzando con la densidad predictiva posterior logarítmica (lppd) y luego agregando un factor de corrección para el número efectivo de parámetros para ajustar por sobreajuste.

También existe el llamado ***Bayesian Information Criterion*** que se calcula a partir del número de parámetros ajustados con una penalización que aumenta con el tamaño de la muestra:
$$
\text{BIC} =  -2\,\text{log}\,p(\boldsymbol{y}\mid\hat{\boldsymbol{\theta}}_{\text{Bayes}}) + k\,\text{log}(n)
$$
donde $k$ es el número de parámetros del modelo y $n$ es el tamaño de la muestra.

El BIC difiere del DIC en que no está motivado por una estimación del ajuste predictivo sino por el objetivo de aproximar la densidad de probabilidad marginal de los datos, $p(\boldsymbol{y})$.

# Ejemplo: Puntajes de Matemáticas

Los conjuntos de datos de los archivos `SB11_1.txt` contiene el **código del departamento de ubicación del colegio** y el **puntaje de matemáticas de los estudiantes que presentaron la Prueba Saber 11 del primer semestre de 2020**. Estos datos son de carácter público y están disponibles en https://www.icfes.gov.co. 
Se recomienda consultar la *Guía de orientación Saber 11* (Icfes, 2020) para obtener más detalles sobre la prueba. 

La prueba de matemáticas se obtiene mediante un **modelo 3PL** (modelo logístico de 3 parámetros que define la probabilidad de responder correctamente de un individuo, en función de su habilidad, la dificultad del ítem, la discriminación del ítem y el pseudo-azar) y tiene una **escala de 0 a 100** (sin decimales), con **puntaje promedio de 50 puntos** y **desviación estándar 10 puntos** (Icfes, 2018, *Guía de diseño, producción, aplicación y calificación del examen Saber 11*, p. 33).

El objetivo es **construir un modelo predictivo para el puntaje de matemáticas a nivel nacional**, tomando como datos de entrenamiento (*training data*) los resultados del primer semestre de 2020 (15435 estudiantes), con el fin de **hacer inferencias sobre la población de estudiantes tanto a nivel nacional como departamental**. Por lo tanto, se toma como variable de agrupamiento el departamento de ubicación del colegio del estudiante. El *Diccionario de variables Saber 11* contiene la información detallada sobre las variables de las bases de datos.

## Estructura de los datos

```{r}
# settings
suppressMessages(suppressWarnings(library(dplyr))) 
suppressMessages(suppressWarnings(library(ggplot2))) 
# datos
SB11_1 <- read.csv("SB11_1.txt", sep=";")
colnames(SB11_1) <- c("DPTO","PUNTAJE")
# tamaños, respuesta, estadisticos
# y    : puntajes de los estudiantes (c)
# Y    : puntajes  de los estudiantes (list)
# g    : identificador de los departamentos (c)
# m    : numero de grupos (departamentos)
# n    : numero de estudiantes en cada departamento (c)
# ybar : promedios de los puntajes (c)
# s2   : varianzas de los puntajes (c)
m <- length(table(SB11_1$DPTO))
m
nj <- as.numeric(table(SB11_1$DPTO))
nj
n <- sum(nj)
n
# respuesta por grupos
y    <- SB11_1$PUNTAJE
ids  <- 1:m
ids2 <- c("Ant","Atl","Bog","Bol","Boy","Cal","Caq","Cau","Ces","Cor","Cun","Hui","Gua","Mag","Met","Nar","NSa","Qui","Ris","San","Tol","VaC","Ara","Cas","Put")
ybar <- NULL
s2   <- NULL
g    <- rep(NA,n)
Y    <- vector(mode = "list", length = m)
for (j in 1:m) {
  index    <- SB11_1$DPTO == unique(SB11_1$DPTO)[j]
  ybar[j]  <- mean(y[index])
  s2[j]    <- var (y[index])
  g[index] <- j
  Y[[j]]   <- y[index]
}
```


## Modelamiento

- M1: Modelo jerárquico Normal con medias específicas.

- M2: Modelo jerárquico Normal con medias y varianzas específicas.

```{r}
# hiperparametros
mu0  <- 50 
g20  <- 25
eta0 <- 1  
t20  <- 100
nu0  <- 1  
s20  <- 100
# almacenamiento
S <- 10000
THETA1 <- matrix(data = NA, nrow = S, ncol = m+3)
LP1 <- NULL
# valores iniciales
theta <- ybar
sig2  <- var(y)
mu    <- mean(theta)
tau2  <- var(theta)
# MCMC
tictoc::tic()
set.seed(1)
for (s in 1:S) {
  # actualizar theta
  vtheta <- 1/(1/tau2 + nj/sig2)
  theta  <- rnorm(n = m, mean = vtheta*(mu/tau2 + nj*ybar/sig2), sd = sqrt(vtheta))
  # actualizar sigma^2
  sig2 <- 1/rgamma(n = 1, shape = 0.5*(nu0 + n), rate = 0.5*(nu0*s20 + sum((nj-1)*s2 + nj*(ybar - theta)^2)))
  # actualizar mu
  vmu <- 1/(1/g20 + m/tau2)
  mu  <- rnorm(n = 1, mean = vmu*(mu0/g20 + m*mean(theta)/tau2), sd = sqrt(vmu)) 
  # actualizar tau^2
  tau2 <- 1/rgamma(n = 1, shape = 0.5*(eta0 + m), rate = 0.5*(eta0*t20 + (m-1)*var(theta) + m*(mean(theta) - mu)^2))
  # almacenar valores
  THETA1[s,] <- c(theta, sig2, mu, tau2)
  # log-verosimilitud
  LP1[s] <- sum(dnorm(x = y, mean = rep(theta, nj), sd = sqrt(sig2), log = T))
}
THETA1 <- as.data.frame(THETA1)
colnames(THETA1) <- c(paste0("theta", 1:m),"sig2","mu","tau2")
tictoc::toc()
# FINAL DEL ALGORITMO
```

```{r}
# hiperparametros
mu0  <- 50 
g20  <- 25
eta0 <- 1  
t20  <- 100
lam0 <- 1  
al0  <- 1
be0  <- 1/100 
nus0 <- 1:50  # rango en p(nu | rest)
# almacenamiento
S <- 10000
THETA2 <- matrix(data = NA, nrow = S, ncol = 2*m+4)
LP2 <- NULL
# valores iniciales
theta <- ybar
sig2  <- s2  # sigma_j^2
mu    <- mean(theta)
tau2  <- var(theta)
nu    <- 1
ups2  <- 100  # sigma^2
# MCMC
tictoc::tic()
set.seed(1)
for (s in 1:S) {
  # actualizar theta
  vtheta <- 1/(1/tau2 + nj/sig2)
  theta  <- rnorm(n = m, mean = vtheta*(mu/tau2 + nj*ybar/sig2), sd = sqrt(vtheta))
  # actualizar sigma_j^2
  sig2 <- 1/rgamma(n = m, shape = 0.5*(nu + nj), rate = 0.5*(nu*ups2 + (nj-1)*s2 + nj*(ybar - theta)^2))
  # actualizar mu
  vmu <- 1/(1/g20 + m/tau2)
  mu  <- rnorm(n = 1, mean = vmu*(mu0/g20 + m*mean(theta)/tau2), sd = sqrt(vmu))
  # actualizar tau2
  tau2 <- 1/rgamma(n = 1, shape = 0.5*(eta0+m), rate = 0.5*(eta0*t20 + (m-1)*var(theta) + m*(mean(theta) - mu)^2))
  # actualizar nu
  lpnu <- 0.5*m*nus0*log(0.5*nus0*ups2) - m*lgamma(0.5*nus0) - 0.5*nus0*sum(log(sig2)) - nus0*(lam0 + 0.5*ups2*sum(1/sig2))
  nu <- sample(x = nus0, size = 1, prob = exp(lpnu - max(lpnu)))
  # actualizar sigma^2
  ups2 <- rgamma(n = 1, shape = al0 + 0.5*m*nu, rate = be0 + 0.5*nu*sum(1/sig2))
  # almacenar
  THETA2[s,] <- c(theta, sig2, mu, tau2, nu, ups2)
  # log-verosimilitud
  LP2[s] <- sum(dnorm(x = y, mean = rep(theta, nj), sd = sqrt(rep(sig2, nj)), log = T))
}
THETA2 <- as.data.frame(THETA2)
colnames(THETA2) <- c(paste0("theta", 1:m), paste0("sig2", 1:m), "mu", "tau2", "nu", "ups2")
tictoc::toc()
# FINAL DEL ALGORITMO
```
## Log-verosimilitud

```{r}
# gráfico log-verosimilitud
par(mfrow=c(1,1),mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(x = NA, y = NA, xlab = "Iteración", ylab = "Log-verosimilitud", cex.axis = 0.7, xlim = c(1, S), ylim = range(LP1, LP2)) 
lines(x = 1:S, y = LP1, type = "l", col = "mistyrose")
lines(x = 1:S, y = LP2, type = "l", col = "lightblue")
abline(h = mean(LP1), lty = 2, col = "red")
abline(h = mean(LP2), lty = 2, col = "blue")
```


```{r}
# gráfico log-verosimilitud
par(mfrow=c(1,1),mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(x = NA, y = NA, ylab = "Densidad", xlab = "Log-verosimilitud", cex.axis = 0.7, xlim = range(LP1, LP2), ylim = c(0,0.12)) 
hist(LP1, freq = F, add = T, col = "mistyrose", border = "mistyrose")
hist(LP2, freq = F, add = T, col = "lightblue", border = "lightblue")
lines(density(LP1), col = "red")
lines(density(LP2), col = "blue")
abline(v = mean(LP1), lty = 2, col = "red")
abline(v = mean(LP2), lty = 2, col = "blue")
legend("top", legend = c("Modelo 1","Modelo 2"), fill = c(2,4), border = c(2,4), bty = "n")
```

## Criterios de información

```{r}
# DIC M1
theta_hat  <- colMeans(THETA1[,1:m])
sigma2_hat <- mean(THETA1$sig2)
lpyth_m1   <- sum(dnorm(x = y, mean = rep(theta_hat, nj), sd = sqrt(sigma2_hat), log = T))
pDIC_m1    <- 2*(lpyth_m1 - mean(LP1))
dic_m1     <- -2*lpyth_m1 + 2*pDIC_m1 
# WAIC M1
lppd_m1  <- 0
pWAIC_m1 <- 0
for (i in 1:n) {
  # lppd
  tmp1    <- dnorm(x = y[i], mean = THETA1[,g[i]], sd = sqrt(THETA1$sig2))
  lppd_m1 <- lppd_m1 + log(mean(tmp1))
  # pWAIC
  tmp2 <- dnorm(x = y[i], mean = THETA1[,g[i]], sd =  sqrt(THETA1$sig2), log = T)
  pWAIC_m1 <- pWAIC_m1 + 2*(log(mean(tmp1)) - mean(tmp2))
}
waic_m1 <- -2*lppd_m1 + 2*pWAIC_m1
# BIC M1
k_m1 <- m + 3
bic_m1 <- -2*lpyth_m1 + k_m1*log(n)
```

```{r}
# DIC M2
theta_hat  <- colMeans(THETA2[,1:m])
sigma2_hat <- colMeans(THETA2[,(m+1):(2*m)])
lpyth_m2   <- sum(dnorm(x = y, mean = rep(theta_hat, nj), sd = sqrt(rep(sigma2_hat, nj)), log = T))
pDIC_m2    <- 2*(lpyth_m2 - mean(LP2))
dic_m2     <- -2*lpyth_m2 + 2*pDIC_m2
# WAIC M2
lppd_m2  <- 0
pWAIC_m2 <- 0
for (i in 1:n) {
  # lppd
  tmp1    <- dnorm(x = y[i], mean = THETA2[,g[i]], sd = sqrt(THETA2[,m+g[i]]))
  lppd_m2 <- lppd_m2 + log(mean(tmp1))
  # pWAIC
  tmp2 <- dnorm(x = y[i], mean = THETA2[,g[i]], sd = sqrt(THETA2[,m+g[i]]), log = T)
  pWAIC_m2 <- pWAIC_m2 + 2*(log(mean(tmp1)) - mean(tmp2))
}
waic_m2 <- -2*lppd_m2 + 2*pWAIC_m2
# BIC M2
k_m2 <- 2*m + 4
bic_m2 <- -2*lpyth_m2 + k_m2*log(n)
```
```{r}
tab <- matrix(c(lpyth_m1, lpyth_m2,
                pDIC_m1,  pDIC_m2,
                dic_m1,   dic_m2,
                lppd_m1,  lppd_m2,
                pWAIC_m1, pWAIC_m2,
                waic_m1,  waic_m2,
                bic_m1,   bic_m2), nrow = 7, ncol = 2, byrow = T)
colnames(tab) <- c("M1","M2")
rownames(tab) <- c("lp","pDIC","DIC","lppd","pWAIC","WAIC","BIC")
knitr::kable(x = tab, digits = 1, align = "c")
```

## Bondad de ajuste departamento máximo

```{r}
# departamento max
j <- which.max(nj)
j
ids2[j]
nj[j]
# datos departamento max
yj <- Y[[j]]
# estadisticos de prueba
ts_disp <- c("MEDIA","SD")
ts_obs <- c(mean(yj),sd(yj))
ts_obs
```

```{r}
# distribucion predictiva
TS1 <- matrix(NA, S, length(ts_obs))
TS2 <- matrix(NA, S, length(ts_obs))
for (s in 1:S) {
  # M1
  yrep    <- rnorm(n = nj[j], mean = THETA1[s,j], sd = sqrt(THETA1$sig2[s]))
  TS1[s,] <- as.numeric(c(mean(yrep), sd(yrep)))
  # M2
  yrep    <- rnorm(n = nj[j], mean = THETA2[s,j], sd = sqrt(THETA2[s,m+j]))
  TS2[s,] <- as.numeric(c(mean(yrep), sd(yrep)))
}
```

```{r, fig.width=10, fig.height=5}
# grafico de los estadisticos de prueba
par(mfrow=c(1,2),mar=c(3,3,1.5,1),mgp=c(1.75,.75,0))
for (i in 1:length(ts_obs)) {
  ts1 <- TS1[,i]
  ts2 <- TS2[,i]
  den1 <- density(ts1, adjust = 1.5)
  den2 <- density(ts2, adjust = 1.5)
  plot(NA, NA, xlim = range(ts1,ts2), ylim = c(0, max(den1$y,den2$y)), xlab = ts_disp[i], ylab = "Densidad", main = ts_disp[i])
  lines(den1, col = 2)
  lines(den2, col = 4)
  abline(v = ts_obs[i], col = "gray", lwd = 2)
  if(i == 1) legend("topright", legend = c("Modelo 1","Modelo 2"), fill = c(2,4), border = c(2,4), bty = "n")
}
```