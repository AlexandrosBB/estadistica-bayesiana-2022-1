---
title: "Modelo Normal Jerárquico 2: Medias y Varianzas Específicas"
author: 
- Juan Sosa PhD
- Webpage https://sites.google.com/view/juansosa/ 
- YouTube https://www.youtube.com/c/JuanSosa1702 
- GitHub  https://github.com/jstats1702 
- Rpubs   https://rpubs.com/jstats1702
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

El tipo más simple de **datos multinivel** tiene **dos niveles**, en los que un nivel consiste de **grupos** y el otro consiste en **unidades dentro de los grupos**. 

Se denota con $y_{i,j}$ la observación de la unidad $i$ en el grupo $j$, para $i = 1,\ldots,n_j$ y $j=1,\ldots,m$, donde $m$ es el número de grupos y $n = \sum_{j=1}^m n_j$ es el tamaño de la muestra.

El **conjunto de datos** es $\boldsymbol{y} = (\boldsymbol{y}_1,\ldots,\boldsymbol{y}_m)$, donde $\boldsymbol{y}_j=(y_{1,j},\ldots,y_{n_j,j})$ son las observaciones asociadas con el grupo $j$, para $j=1,\ldots,m$.

# Modelo

Un modelo popular para describir la **heterogeneidad de las medias** en varias poblaciones es el modelo jerárquico normal, en el cual la **variabilidad dentro y entre** se modela usando **distribuciones normales**:

- Caracterización **dentro** de los grupos:
$$
y_{i, j}\mid\theta_j,\sigma^2 \stackrel{\text {iid}}{\sim} \textsf{N}\left(\theta_j,\sigma_j^2\right)
$$
- Caracterización **entre** los grupos:
$$
\theta_{j}\mid\mu,\tau^2 \stackrel{\text {iid}}{\sim} \textsf{N}\left(\mu,\tau^2\right)
$$
$$
\sigma_j^2\mid\nu,\sigma^2 \stackrel{\text {iid}}{\sim}\textsf{GI}\left(\tfrac{\nu}{2},\tfrac{\nu\,\sigma^2}{2}\right)
$$

- Distribución **previa**:
$$
p(\mu,\tau^2,\nu_0,\sigma^2_0) = p(\mu)\,p(\tau^2)\,p(\nu)\,p(\sigma^2)
$$
donde
$$
\mu\sim\textsf{N}(\mu_0,\gamma_0^2)\qquad\tau^2\sim\textsf{GI}\left(\tfrac{\eta_0}{2},\tfrac{\eta_0\,\tau^2_0}{2}\right)
\qquad p(\nu)\propto e^{-\lambda_0\,\nu}\qquad \sigma^2\sim\textsf{Gamma}(\alpha_0,\beta_0)
$$
- Los **parámetros** del modelo son $\boldsymbol{\theta}=(\theta_1,\ldots,\theta_m,\sigma^2_1,\ldots,\sigma^2_m,\mu,\tau^2,\nu,\sigma^2)$.

- Los **hiper-parámetros** del modelo son $(\mu_0,\gamma_0^2,\eta_0,\tau^2_0, \lambda_0,\alpha_0,\beta_0)$.

```{r, eval = TRUE, echo=FALSE, out.width="50%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("modelo_normal_jerarquico_2.png")
```

# Estimación

Construir un **muestreador de Gibbs** para obtener muestras de la distribución posterior $p(\boldsymbol{\theta}\mid\boldsymbol{y})$.

- Distribución **posterior**:
$$
\begin{aligned}
p(\boldsymbol{\theta} \mid \boldsymbol{y}) &\propto p(\boldsymbol{y} \mid \boldsymbol{\theta})\, p(\boldsymbol{\theta}) \\
&=  p(\boldsymbol{y}_1,\ldots,\boldsymbol{y}_m \mid \theta_1,\ldots\theta_m,\sigma_1^2,\ldots,\sigma^2_m)\times p(\theta_1,\ldots,\theta_m\mid\mu,\tau^2)\times p(\sigma^2_1,\ldots,\sigma^2_m\mid\nu,\sigma^2) \times p(\mu,\tau^2,\nu,\sigma^2) \\
&=\prod_{j=1}^m\prod_{i=1}^{n_j} \textsf{N}\left(y_{i, j} \mid \theta_{j}, \sigma^{2}\right) \times \prod_{j=1}^m \textsf{N}\left(\theta_{j} \mid \mu, \tau^{2}\right) \times \prod_{j=1}^m \textsf{GI}\left(\sigma^2_{j} \mid \tfrac{\nu}{2}, \tfrac{\nu\,\sigma^{2}}{2}\right)  \times \textsf{N}\left(\mu \mid \mu_{0}, \gamma_{0}^{2}\right) \times \textsf{GI}\left(\tau^{2} \mid \tfrac{\eta_{0}}{2}, \tfrac{\eta_{0}\,\tau_{0}^{2}}{2}\right)\times e^{-\lambda_0\,\nu} \times \textsf{G}(\sigma^2\mid\alpha_0,\beta_0)
\end{aligned}
$$

Distribuciones **condicionales completas**:
$$
\begin{aligned}
p\left(\theta_{j} \mid \text { resto }\right) &=\textsf{N}\left(\theta_{j} \,\Big|\, \frac{\mu / \tau^{2} + n_{j} \bar{y}_{j} / \sigma_j^{2}}{1 / \tau^{2} + n_{j} /\sigma_j^{2}}, \frac{1}{1 / \tau^{2} + n_{j} /\sigma_j^{2}}\right) \\
p\left(\sigma_{j}^{2} \,\Big|\, \text { resto }\right) &=\textsf{GI}\left(\sigma_j^{2} \mid \frac{\nu+n_{j}}{2}, \frac{\nu \sigma^{2}+\sum_{i=1}^{n_j}\left(y_{i, j}-\theta_{j}\right)^{2}}{2}\right) \\
p(\mu \mid \text { resto }) &=\textsf{N}\left(\mu \,\Big|\, \frac{\mu_{0} / \gamma_{0}^{2} + m \bar{\theta} / \tau^{2}}{1 / \gamma_{0}^{2} + m / \tau^{2}}, \frac{1}{1 / \gamma_{0}^{2} + m / \tau^{2}}\right) \\
p\left(\tau^{2} \mid \text { resto }\right) &=\textsf{ GI }\left(\tau^{2} \,\Big|\, \frac{\eta_{0}+m}{2}, \frac{\eta_{0} \tau_{0}^{2}+\sum_{j=1}^m\left(\theta_{j}-\mu\right)^{2}}{2}\right)\\
p\left(\sigma^{2} \mid \text { resto }\right) &=\textsf{G}\left(\sigma^{2} \,\Big|\, \alpha_0+\frac{m \nu}{2},  \beta_0+\frac{\nu}{2} \sum_{j=1}^m \frac{1}{\sigma_{j}^2}\right) 
\end{aligned}
$$
La distribución condicional completa de $\nu$ no tiene una forma cerrada conocida:
$$
p\left(\nu \mid \text { resto }\right) \propto\left[\frac{\left(\nu\,\sigma^{2} / 2\right)^{\nu / 2}}{\Gamma\left(\nu / 2\right)}\right]^{m}\left[\prod_{j=1}^m \frac{1}{\sigma_j^{2}}\right]^{\nu / 2} {\exp}\left\{-\nu\left(\lambda_0 + \frac{\sigma^{2}}{2} \sum_{j=1}^m \frac{1}{\sigma_{j}^{2}}\right)\right\}
$$
Para muestrear de $p(\nu \mid \text{resto})$ se calcula esta distribución en escala log para un rango de valores de $\nu$, se normaliza la distribución discreta resultante, y luego se simula de rango de valores de acuerdo con las probabilidades obtenidas.
$$
\log p\left(\nu \mid \text { resto }\right) \propto \frac{m\,\nu}{2} \log(\nu\,\sigma^{2} / 2) - m\log\Gamma(\nu/2) -\frac{\nu}{2} \sum_{j=1}^m \log(\sigma_j^{2}) - \nu\left(\lambda_0 + \frac{\sigma^{2}}{2} \sum_{j=1}^m \frac{1}{\sigma_{j}^{2}}\right)
$$

- Si $\nu_0 = 1$, entonces todas las intra-varianzas son disimiles y se comparte muy poca información acerca de las varianzas particulares a través de los grupos.
- Si $\nu \rightarrow \infty$, entonces este modelo es igual al anterior donde todas las intra-varianzas son iguales. 

# Ejemplo: Puntajes de Matemáticas

Los conjuntos de datos de los archivos `SB11_1.txt` contiene el **código del departamento de ubicación del colegio** y el **puntaje de matemáticas de los estudiantes que presentaron la Prueba Saber 11 del primer semestre de 2020**. Estos datos son de carácter público y están disponibles en https://www.icfes.gov.co. 
Se recomienda consultar la *Guía de orientación Saber 11* (Icfes, 2020) para obtener más detalles sobre la prueba. 

La prueba de matemáticas se obtiene mediante un **modelo 3PL** (modelo logístico de 3 parámetros que define la probabilidad de responder correctamente de un individuo, en función de su habilidad, la dificultad del ítem, la discriminación del ítem y el pseudo-azar) y tiene una **escala de 0 a 100** (sin decimales), con **puntaje promedio de 50 puntos** y **desviación estándar 10 puntos** (Icfes, 2018, *Guía de diseño, producción, aplicación y calificación del examen Saber 11*, p. 33).

El objetivo es **construir un modelo predictivo para el puntaje de matemáticas a nivel nacional**, tomando como datos de entrenamiento (*training data*) los resultados del primer semestre de 2020 (15435 estudiantes), con el fin de **hacer inferencias sobre la población de estudiantes tanto a nivel nacional como departamental**. Por lo tanto, se toma como variable de agrupamiento el departamento de ubicación del colegio del estudiante. El *Diccionario de variables Saber 11* contiene la información detallada sobre las variables de las bases de datos.

## Estructura de los datos

```{r}
# datos
SB11_1 <- read.csv("SB11_1.txt", sep=";")
colnames(SB11_1) <- c("DPTO","PUNTAJE")
# tamaños, respuesta, estadisticos
# y    : puntajes de los estudiantes (c)
# Y    : puntajes  de los estudiantes (list)
# g    : identificador de los departamentos (c)
# m    : numero de grupos (departamentos)
# n    : numero de estudiantes en cada departamento (c)
# ybar : promedios de los puntajes (c)
# s2   : varianzas de los puntajes (c)
m <- length(table(SB11_1$DPTO))
m
nj <- as.numeric(table(SB11_1$DPTO))
nj
n <- sum(nj)
n
# respuesta por grupos
y    <- SB11_1$PUNTAJE
ids  <- 1:m
ids2 <- c("Ant","Atl","Bog","Bol","Boy","Cal","Caq","Cau","Ces","Cor","Cun","Hui","Gua","Mag","Met","Nar","NSa","Qui","Ris","San","Tol","VaC","Ara","Cas","Put")
ybar <- NULL
s2   <- NULL
g    <- rep(NA,n)
Y    <- vector(mode = "list", length = m)
for (j in 1:m) {
  index    <- SB11_1$DPTO == unique(SB11_1$DPTO)[j]
  ybar[j]  <- mean(y[index])
  s2[j]    <- var (y[index])
  g[index] <- j
  Y[[j]]   <- y[index]
}
```


## Modelo jerárquico Normal con medias específicas

**Distribución previa jerárquica:**
$$
\theta_{j}\mid\mu,\tau^2 \stackrel{\text {iid}}{\sim} \textsf{N}\left(\mu,\tau^2\right)\,,
$$
$$
\sigma_j^2\mid\nu,\sigma^2 \stackrel{\text {iid}}{\sim}\textsf{GI}\left(\tfrac{\nu}{2},\tfrac{\nu\,\sigma^2}{2}\right)\,,
$$
y 
$$
p(\mu,\tau^2,\nu_0,\sigma^2_0) = p(\mu)\,p(\tau^2)\,p(\nu)\,p(\sigma^2)\,,
$$
con
$$
\mu\sim\textsf{N}(\mu_0,\gamma_0^2)\,,\qquad\tau^2\sim\textsf{GI}\left(\tfrac{\eta_0}{2},\tfrac{\eta_0\,\tau^2_0}{2}\right)\,,
\qquad p(\nu)\propto e^{-\lambda_0\,\nu}\,,\qquad \sigma^2\sim\textsf{Gamma}(\alpha_0,\beta_0)\,.
$$

Teniendo en cuenta la información de la prueba, el modelo se ajusta utilizando los siguientes hiperparámetros: 
$$
\mu_0 = 50\,,\qquad
\gamma^2_0 = 25\,,\qquad
\eta_0 = 1\,,\qquad
\tau^2_0 = 100\,,\qquad
\lambda_0 = 1\,,\qquad
\alpha_0 = 1\,,\qquad
\beta_0 = 1/100\,.
$$

## Estimación

```{r}
# hiperparametros
mu0  <- 50 
g20  <- 25
eta0 <- 1  
t20  <- 100
nu0  <- 1  
s20  <- 100
# almacenamiento
S <- 10000
THETA0 <- matrix(data = NA, nrow = S, ncol = m+3)
LP <- NULL
# valores iniciales
theta <- ybar
sig2  <- var(y)
mu    <- mean(theta)
tau2  <- var(theta)
# MCMC
tictoc::tic()
set.seed(1)
for (s in 1:S) {
  # actualizar theta
  vtheta <- 1/(1/tau2 + nj/sig2)
  theta  <- rnorm(n = m, mean = vtheta*(mu/tau2 + nj*ybar/sig2), sd = sqrt(vtheta))
  # actualizar sigma^2
  sig2 <- 1/rgamma(n = 1, shape = 0.5*(nu0 + n), rate = 0.5*(nu0*s20 + sum((nj-1)*s2 + nj*(ybar - theta)^2)))
  # actualizar mu
  vmu <- 1/(1/g20 + m/tau2)
  mu  <- rnorm(n = 1, mean = vmu*(mu0/g20 + m*mean(theta)/tau2), sd = sqrt(vmu)) 
  # actualizar tau^2
  tau2 <- 1/rgamma(n = 1, shape = 0.5*(eta0 + m), rate = 0.5*(eta0*t20 + (m-1)*var(theta) + m*(mean(theta) - mu)^2))
  # almacenar valores
  THETA0[s,] <- c(theta, sig2, mu, tau2)
  # log-verosimilitud
  LP[s] <- sum(dnorm(x = y, mean = rep(theta, nj), sd = sqrt(sig2), log = T))
}
THETA0 <- as.data.frame(THETA0)
colnames(THETA0) <- c(paste0("theta", 1:m),"sig2","mu","tau2")
tictoc::toc()
# FINAL DEL ALGORITMO
```

```{r}
# hiperparametros
mu0  <- 50 
g20  <- 25
eta0 <- 1  
t20  <- 100
lam0 <- 1  
al0  <- 1
be0  <- 1/100 
nus0 <- 1:50  # rango en p(nu | rest)
# almacenamiento
S <- 10000
THETA <- matrix(data = NA, nrow = S, ncol = 2*m+4)
LP <- NULL
# valores iniciales
theta <- ybar
sig2  <- s2  # sigma_j^2
mu    <- mean(theta)
tau2  <- var(theta)
nu    <- 1
ups2  <- 100  # sigma^2
# MCMC
tictoc::tic()
set.seed(1)
for (s in 1:S) {
  # actualizar theta
  vtheta <- 1/(1/tau2 + nj/sig2)
  theta  <- rnorm(n = m, mean = vtheta*(mu/tau2 + nj*ybar/sig2), sd = sqrt(vtheta))
  # actualizar sigma_j^2
  sig2 <- 1/rgamma(n = m, shape = 0.5*(nu + nj), rate = 0.5*(nu*ups2 + (nj-1)*s2 + nj*(ybar - theta)^2))
  # actualizar mu
  vmu <- 1/(1/g20 + m/tau2)
  mu  <- rnorm(n = 1, mean = vmu*(mu0/g20 + m*mean(theta)/tau2), sd = sqrt(vmu))
  # actualizar tau2
  tau2 <- 1/rgamma(n = 1, shape = 0.5*(eta0+m), rate = 0.5*(eta0*t20 + (m-1)*var(theta) + m*(mean(theta) - mu)^2))
  # actualizar nu
  lpnu <- 0.5*m*nus0*log(0.5*nus0*ups2) - m*lgamma(0.5*nus0) - 0.5*nus0*sum(log(sig2)) - nus0*(lam0 + 0.5*ups2*sum(1/sig2))
  nu <- sample(x = nus0, size = 1, prob = exp(lpnu - max(lpnu)))
  # actualizar sigma^2
  ups2 <- rgamma(n = 1, shape = al0 + 0.5*m*nu, rate = be0 + 0.5*nu*sum(1/sig2))
  # almacenar
  THETA[s,] <- c(theta, sig2, mu, tau2, nu, ups2)
  # log-verosimilitud
  LP[s] <- sum(dnorm(x = y, mean = rep(theta, nj), sd = sqrt(rep(sig2, nj)), log = T))
}
THETA <- as.data.frame(THETA)
colnames(THETA) <- c(paste0("theta", 1:m), paste0("sig2", 1:m), "mu", "tau2", "nu", "ups2")
tictoc::toc()
# FINAL DEL ALGORITMO
```

## Convergencia

```{r, fig.width=10, fig.height=5}
# cadenas
par(mfrow=c(1,1),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
plot(LP, xlab="Iteracion", type ="l", ylab="Log-verosimilitud")
```

```{r}
# tamaños efectivos de muestra
summary(coda::effectiveSize(THETA))
```

## Inferencia

```{r, fig.width=10, fig.height=10}
# graficos
par(mfrow=c(2,2),mar=c(3,3,1,1),mgp=c(1.75,.75,0))
# posterior mu
plot(density(THETA$mu,adj=2),lwd=2,main="",xlab=expression(mu), ylab=expression(paste(italic("p("),mu,"|",italic(y[1]),"...",italic(y[m]),")")))
lines(density(THETA0$mu,adj=2), col = "green")
abline(v=quantile(THETA$mu, c(.025,.5,.975)), col=c(2,4,2), lty=c(3,2,3))
# posterior tau^2
plot(density(sqrt(THETA$tau2),adj=2),lwd=2,main="",xlab=expression(tau), ylab=expression(paste(italic("p("),tau,"|",italic(y[1]),"...",italic(y[m]),")")))
lines(density(sqrt(THETA0$tau2),adj=2), col = "green")
abline(v=quantile(sqrt(THETA$tau2), c(.025,.5,.975)), col=c(2,4,2), lty=c(3,2,3))
# posterior nu
plot(table(THETA$nu),xlab=expression(nu[0]), ylab=expression(paste(italic("p("),nu,"|",italic(y[1]),"...",italic(y[m]),")")))
# posterior sigma^2
plot(density(sqrt(THETA$ups2),adj=2),lwd=2,main="",xlab=expression(sigma), ylab=expression(paste(italic("p("),sigma,"|",italic(y[1]),"...",italic(y[m]),")")))
abline(v=quantile(sqrt(THETA$ups2), c(.025,.5,.975)), col=c(2,4,2), lty=c(3,2,3))
```

## Ranking

```{r, fig.width=10, fig.height=5}
ids  <- 1:m
ids2 <- c("Ant","Atl","Bog","Bol","Boy","Cal","Caq","Cau","Ces","Cor","Cun","Hui","Gua","Mag","Met","Nar","NSa","Qui","Ris","San","Tol","VaC","Ara","Cas","Put")
that <- colMeans(THETA[,1:m])
ic1  <- apply(X = THETA[,1:m], MARGIN = 2, FUN = function(x) quantile(x, c(0.025,0.975)))
ranking <- order(that, decreasing = T) 
ids  <- ids [ ranking]
ids2 <- ids2[ ranking]
that <- that[ ranking]
ic1  <- ic1 [,ranking]
colo <- rep(2,m)
colo[which(ic1[1,]>50)] <- 1
colo[which(ic1[2,]<50)] <- 3
colo <- c("royalblue","black","red")[colo]
# grafico
par(mfrow=c(1,1),mar=c(4,4,1.5,1),mgp=c(2.5,.75,0))
plot(NA, NA, xlab = "Departamento", ylab = "Puntaje", main = paste0("Ranking"), xlim = c(1,m), ylim = range(THETA[,1:m]), cex.axis = 0.75, xaxt = "n")
axis(side = 1, at = 1:m, labels = F)
text(x = (1:m) + 0.3, y = par("usr")[3] - 3, labels = ids2, srt = 70, pos = 2, xpd = T, cex = 0.75)
abline(v = 1:m, col = "gray95", lwd = 1, lty = 3)
abline(h = 50,  col = "gray85", lwd = 2, lty = 1)
for (j in 1:m) {
  segments(x0 = j, y0 = ic1[1,j], x1 = j, y1 = ic1[2,j], lwd = 1, col = colo[j])
  lines(x = j, y = that[j], type = "p", pch = 16, cex = 0.8, col = colo[j])
}
```

```{r, fig.width=10, fig.height=5}
ids  <- 1:m
ids2 <- c("Ant","Atl","Bog","Bol","Boy","Cal","Caq","Cau","Ces","Cor","Cun","Hui","Gua","Mag","Met","Nar","NSa","Qui","Ris","San","Tol","VaC","Ara","Cas","Put")
that <- apply(X = THETA[,1:m], MARGIN = 2, FUN = mean)
shat <- apply(X = THETA[,1:m], MARGIN = 2, FUN = sd)
cv   <- 100*shat/that
ranking <- order(that, decreasing = T) 
ids  <- ids [ ranking]
ids2 <- ids2[ ranking]
cv   <- cv[ ranking]
# grafico
par(mfrow=c(1,1),mar=c(4,4,1.5,1),mgp=c(2.5,.75,0))
plot(1:m, cv, xlab = "Departamento", ylab = "CV(%)", main = paste0("CV(%)"), pch = 19, type = "b", xlim = c(1,m), ylim = c(0,20), cex.axis = 0.75, xaxt = "n")
axis(side = 1, at = 1:m, labels = F)
text(x = (1:m) + 0.3, y = par("usr")[3] - 1.5, labels = ids2, srt = 70, pos = 2, xpd = T, cex = 0.75)
abline(v = 1:m, col = "gray95", lwd = 1, lty = 3)
abline(h = 5,  lty = 2, col = 3)
abline(h = 10, lty = 2, col = "#FFA500")
abline(h = 15, lty = 2, col = 2)
```

```{r, fig.width=10, fig.height=5}
shat <- colMeans(sqrt(THETA[,(m+1):(2*m)]))
ic1  <- apply(X = sqrt(THETA[,(m+1):(2*m)]), MARGIN = 2, FUN = function(x) quantile(x, c(0.025,0.975)))
shat <- shat[ ranking]
ic1  <- ic1 [,ranking]
# grafico
par(mfrow=c(1,1),mar=c(4,4,1.5,1),mgp=c(2.5,.75,0))
plot(NA, NA, xlab = "Departamento", ylab = "Puntaje", main = paste0("Desv. Estándar"), xlim = c(1,m), ylim = range(sqrt(THETA[,(m+1):(2*m)])), cex.axis = 0.75, xaxt = "n")
axis(side = 1, at = 1:m, labels = F)
text(x = (1:m) + 0.3, y = par("usr")[3] - 2, labels = ids2, srt = 70, pos = 2, xpd = T, cex = 0.75)
abline(v = 1:m, col = "gray95", lwd = 1, lty = 3)
abline(h = 10,  col = "gray85", lwd = 2, lty = 1)
for (j in 1:m) {
  segments(x0 = j, y0 = ic1[1,j], x1 = j, y1 = ic1[2,j], lwd = 1, col = colo[j])
  lines(x = j, y = shat[j], type = "p", pch = 16, cex = 0.8, col = colo[j])
}
```